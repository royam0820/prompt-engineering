{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royam0820/prompt-engineering/blob/main/l5_inferring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3630c235-f891-4874-bd0a-5277d4d6aa82",
      "metadata": {
        "id": "3630c235-f891-4874-bd0a-5277d4d6aa82"
      },
      "source": [
        "# Inferring\n",
        "In this lesson, you will infer sentiment and topics from product reviews and news articles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of prompt engineering, **inferring** refers to the process of guiding an AI model to draw conclusions, make interpretations, or deduce information from a given text or data. This involves crafting prompts that instruct the model to go beyond the explicit content and extract implicit meaning, relationships, or insights. Inferring is useful for tasks such as sentiment analysis, understanding motivations, identifying underlying themes, or predicting outcomes based on provided information.\n",
        "\n"
      ],
      "metadata": {
        "id": "nrJozy1uXJ71"
      },
      "id": "nrJozy1uXJ71"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Aspects of Inferring in Prompt Engineering\n",
        "\n",
        "- **Explicit Instructions**: The prompt should clearly indicate that the model needs to infer information. Using directives like \"infer,\" \"deduce,\" or \"conclude\" helps set the expectation.\n",
        "\n",
        "- **Contextual Understanding**: Inferring often requires the model to consider context. Prompts can include background information or set up scenarios to provide the necessary context for accurate inference.\n",
        "\n",
        "- **Specific Queries**: The prompts can ask specific questions that require inference, such as \"What is the sentiment of the following review?\" or \"Why did the character make this decision?\"\n",
        "\n",
        "- **Evidence-Based Reasoning**: The inference should be based on evidence from the text. Prompts can guide the model to explain its reasoning or cite parts of the text that support its inference."
      ],
      "metadata": {
        "id": "HfLq-HdEXTWQ"
      },
      "id": "HfLq-HdEXTWQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applications of Inferring in AI\n",
        "- **Sentiment Analysis**: Determining the emotional tone of a piece of text (e.g., positive, negative, neutral).\n",
        "- **Motivation and Intent**: Understanding the reasons behind a character's or person's actions in a narrative.\n",
        "- **Theme Identification**: Recognizing underlying themes or messages in a text.\n",
        "- **Predictive Analysis**: Making predictions based on patterns or trends identified in the data."
      ],
      "metadata": {
        "id": "NH1IJkyDX8Yg"
      },
      "id": "NH1IJkyDX8Yg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tips for Effective Inferring Prompts\n",
        "- Clarity and Precision: Be clear and precise about what needs to be inferred.\n",
        "- Provide Sufficient Context: Ensure the model has enough information to make accurate inferences.\n",
        "- Guide Reasoning: Encourage the model to explain its inferences or provide supporting evidence.\n",
        "- Iterate and Refine: Test and refine prompts to improve the accuracy and relevance of the inferences made by the model.\n"
      ],
      "metadata": {
        "id": "jabbBPqIYZ0O"
      },
      "id": "jabbBPqIYZ0O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "1VL1vEva97yI"
      },
      "id": "1VL1vEva97yI"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ued_418AogZz",
        "outputId": "a113029b-a885-479e-b100-c8f53c2e66d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.35.3-py3-none-any.whl (327 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.35.3\n"
          ]
        }
      ],
      "id": "Ued_418AogZz"
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "m39DjFvJoI0H"
      },
      "execution_count": 3,
      "outputs": [],
      "id": "m39DjFvJoI0H"
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "FBexQysn5fl5"
      },
      "execution_count": 4,
      "outputs": [],
      "id": "FBexQysn5fl5"
    },
    {
      "cell_type": "code",
      "source": [
        "print(openai.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dswy5j-noziQ",
        "outputId": "0c763565-2c59-4f2e-cf33-6d474697eff3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.35.3\n"
          ]
        }
      ],
      "id": "dswy5j-noziQ"
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ],
      "metadata": {
        "id": "JzhXT5sAnTLA"
      },
      "execution_count": 6,
      "outputs": [],
      "id": "JzhXT5sAnTLA"
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI()\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "47w3YM77pZG6"
      },
      "execution_count": 7,
      "outputs": [],
      "id": "47w3YM77pZG6"
    },
    {
      "cell_type": "markdown",
      "id": "51d2fdfa-c99f-4750-8574-dba7712cd7f0",
      "metadata": {
        "id": "51d2fdfa-c99f-4750-8574-dba7712cd7f0"
      },
      "source": [
        "## Product review text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b0f3b49b",
      "metadata": {
        "height": 200,
        "id": "b0f3b49b"
      },
      "outputs": [],
      "source": [
        "# produc review about a lamp\n",
        "lamp_review = \"\"\"\n",
        "Needed a nice lamp for my bedroom, and this one had \\\n",
        "additional storage and not too high of a price point. \\\n",
        "Got it fast.  The string to our lamp broke during the \\\n",
        "transit and the company happily sent over a new one. \\\n",
        "Came within a few days as well. It was easy to put \\\n",
        "together.  I had a missing part, so I contacted their \\\n",
        "support and they very quickly got me the missing piece! \\\n",
        "Lumina seems to me to be a great company that cares \\\n",
        "about their customers and products!!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30d6e4bd-3337-45a3-8c99-a734cdd06743",
      "metadata": {
        "id": "30d6e4bd-3337-45a3-8c99-a734cdd06743"
      },
      "source": [
        "## Sentiment (positive/negative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "e3157601",
      "metadata": {
        "height": 149,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3157601",
        "outputId": "ca0a1e44-344d-400c-f205-57f7d0411033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('The sentiment of the review is positive. The reviewer is satisfied with the '\n",
            " 'lamp they purchased, mentioning the additional storage, fast delivery, '\n",
            " 'excellent customer service, and ease of assembly. They also praise the '\n",
            " 'company for caring about their customers and products.')\n"
          ]
        }
      ],
      "source": [
        "# requesting sentiment analysis in the prompt\n",
        "prompt = f\"\"\"\n",
        "What is the sentiment of the following product review,\n",
        "which is delimited with triple backticks?\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "pprint(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "acf9ca16",
      "metadata": {
        "height": 200,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acf9ca16",
        "outputId": "9b4b30d8-87d6-4baa-c7a5-aa66341630c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ],
      "source": [
        "# extracting ony the sentiment anslysis\n",
        "prompt = f\"\"\"\n",
        "What is the sentiment of the following product review,\n",
        "which is delimited with triple backticks?\n",
        "\n",
        "Give your answer as a single word, either \"positive\" \\\n",
        "or \"negative\".\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81d2a973-1fa4-4a35-ae35-a2e746c0e91b",
      "metadata": {
        "id": "81d2a973-1fa4-4a35-ae35-a2e746c0e91b"
      },
      "source": [
        "## Identify types of emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "8aa7934b",
      "metadata": {
        "height": 183,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aa7934b",
        "outputId": "198290bd-c520-4aa2-9748-bb5e4931c7ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happy, satisfied, grateful, impressed, pleased\n"
          ]
        }
      ],
      "source": [
        "# identify a list of emotions\n",
        "prompt = f\"\"\"\n",
        "Identify a list of emotions that the writer of the \\\n",
        "following review is expressing. Include no more than \\\n",
        "five items in the list. Format your answer as a list of \\\n",
        "lower-case words separated by commas.\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a428d093-51c9-461c-b41e-114e80876409",
      "metadata": {
        "id": "a428d093-51c9-461c-b41e-114e80876409"
      },
      "source": [
        "## Identify anger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "dba1a538",
      "metadata": {
        "height": 166,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dba1a538",
        "outputId": "54efdd0e-4aa1-400b-ca99-fa1abaf8bf7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No\n"
          ]
        }
      ],
      "source": [
        "# identify anger\n",
        "prompt = f\"\"\"\n",
        "Is the writer of the following review expressing anger?\\\n",
        "The review is delimited with triple backticks. \\\n",
        "Give your answer as either yes or no.\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "936a771e-ca78-4e55-8088-2da6f3820ddc",
      "metadata": {
        "id": "936a771e-ca78-4e55-8088-2da6f3820ddc"
      },
      "source": [
        "## Extract product and company name from customer reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "a13bea1b",
      "metadata": {
        "height": 285,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a13bea1b",
        "outputId": "248becdb-bc81-439f-950a-3092ac1e08ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"Item\": \"lamp\",\n",
            "    \"Brand\": \"Lumina\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# extracting information such as the item and the brand of the item\n",
        "prompt = f\"\"\"\n",
        "Identify the following items from the review text:\n",
        "- Item purchased by reviewer\n",
        "- Company that made the item\n",
        "\n",
        "The review is delimited with triple backticks. \\\n",
        "Format your response as a JSON object with \\\n",
        "\"Item\" and \"Brand\" as the keys.\n",
        "If the information isn't present, use \"unknown\" \\\n",
        "as the value.\n",
        "Make your response as short as possible.\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a38880a5-088f-4609-9913-f8fa41fb7ba0",
      "metadata": {
        "id": "a38880a5-088f-4609-9913-f8fa41fb7ba0"
      },
      "source": [
        "## Doing multiple tasks at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "e7dda9e5",
      "metadata": {
        "height": 336,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7dda9e5",
        "outputId": "18f8da4d-385a-471e-fdda-76378c628212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"Sentiment\": \"positive\",\n",
            "    \"Anger\": false,\n",
            "    \"Item\": \"lamp\",\n",
            "    \"Brand\": \"Lumina\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# outputting a json format for sentiment, anger (boolean value), item, and brand.\n",
        "prompt = f\"\"\"\n",
        "Identify the following items from the review text:\n",
        "- Sentiment (positive or negative)\n",
        "- Is the reviewer expressing anger? (true or false)\n",
        "- Item purchased by reviewer\n",
        "- Company that made the item\n",
        "\n",
        "The review is delimited with triple backticks. \\\n",
        "Format your response as a JSON object with \\\n",
        "\"Sentiment\", \"Anger\", \"Item\" and \"Brand\" as the keys.\n",
        "If the information isn't present, use \"unknown\" \\\n",
        "as the value.\n",
        "Make your response as short as possible.\n",
        "Format the Anger value as a boolean.\n",
        "\n",
        "Review text: '''{lamp_review}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: regarding the output, the boolean value is a boolean value and not a string. Useful, if we want to take this json output into a python directory for example."
      ],
      "metadata": {
        "id": "w3qRHDnVjNYX"
      },
      "id": "w3qRHDnVjNYX"
    },
    {
      "cell_type": "markdown",
      "id": "235fc223-2c89-49ec-ac2d-78a8e74a43ac",
      "metadata": {
        "id": "235fc223-2c89-49ec-ac2d-78a8e74a43ac"
      },
      "source": [
        "## Inferring topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "8a74cc3e",
      "metadata": {
        "height": 472,
        "id": "8a74cc3e"
      },
      "outputs": [],
      "source": [
        "# the text of the story\n",
        "story = \"\"\"\n",
        "In a recent survey conducted by the government,\n",
        "public sector employees were asked to rate their level\n",
        "of satisfaction with the department they work at.\n",
        "The results revealed that NASA was the most popular\n",
        "department with a satisfaction rating of 95%.\n",
        "\n",
        "One NASA employee, John Smith, commented on the findings,\n",
        "stating, \"I'm not surprised that NASA came out on top.\n",
        "It's a great place to work with amazing people and\n",
        "incredible opportunities. I'm proud to be a part of\n",
        "such an innovative organization.\"\n",
        "\n",
        "The results were also welcomed by NASA's management team,\n",
        "with Director Tom Johnson stating, \"We are thrilled to\n",
        "hear that our employees are satisfied with their work at NASA.\n",
        "We have a talented and dedicated team who work tirelessly\n",
        "to achieve our goals, and it's fantastic to see that their\n",
        "hard work is paying off.\"\n",
        "\n",
        "The survey also revealed that the\n",
        "Social Security Administration had the lowest satisfaction\n",
        "rating, with only 45% of employees indicating they were\n",
        "satisfied with their job. The government has pledged to\n",
        "address the concerns raised by employees in the survey and\n",
        "work towards improving job satisfaction across all departments.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8ea91d6-e841-4ee2-bed9-ca4a36df177f",
      "metadata": {
        "id": "a8ea91d6-e841-4ee2-bed9-ca4a36df177f"
      },
      "source": [
        "## Infer 5 topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "5c267cbe",
      "metadata": {
        "height": 217,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c267cbe",
        "outputId": "f7d25a65-c1b0-46bd-a85d-f1da9c5caa67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Survey\n",
            "2. Job satisfaction\n",
            "3. NASA\n",
            "4. Social Security Administration\n",
            "5. Government pledge\n"
          ]
        }
      ],
      "source": [
        "# from the story above, we asked the inference of 5 topics\n",
        "prompt = f\"\"\"\n",
        "Determine five topics that are being discussed in the \\\n",
        "following text, which is delimited by triple backticks.\n",
        "\n",
        "Make each item one or two words long.\n",
        "\n",
        "Format your response as a list of items separated by commas.\n",
        "\n",
        "Text sample: '''{story}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "96hkNiXhU6c1"
      },
      "id": "96hkNiXhU6c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "f92f90fe",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f92f90fe",
        "outputId": "919be2a1-df6a-4c94-f3db-8aa0d008ef6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[1', ' 0', ' 0', ' 1', ' 1]']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "response.split(sep=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming, you have a topic list as dislayed below and you would like to know which news article belongs to that topic list. A way to classify your news information."
      ],
      "metadata": {
        "id": "tUxMZD4RlLBN"
      },
      "id": "tUxMZD4RlLBN"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "94b8fa65",
      "metadata": {
        "height": 81,
        "id": "94b8fa65"
      },
      "outputs": [],
      "source": [
        "topic_list = [\n",
        "    \"nasa\", \"local government\", \"engineering\",\n",
        "    \"employee satisfaction\", \"federal government\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34be1d2a-1309-4512-841a-b6f67338938b",
      "metadata": {
        "id": "34be1d2a-1309-4512-841a-b6f67338938b"
      },
      "source": [
        "## Make a news alert for certain topics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Determine five topics that are being discussed in the \\\n",
        "following text, which is delimited by triple backticks.\n",
        "\n",
        "Make each item one or two words long.\n",
        "\n",
        "Format your response as a list of items separated by commas.\n",
        "\n",
        "Text sample: '''{story}'''\n",
        "\"\"\"\n",
        "\n",
        "# Assume get_completion is a function that sends the prompt to the language model and gets the response\n",
        "response = get_completion(prompt)\n",
        "print(\"Response:\", response)\n",
        "\n",
        "# Convert the response to lowercase\n",
        "response_lower = response.lower()\n",
        "\n",
        "# Count occurrences of each topic in the topic_list within the response\n",
        "topic_counts = {topic: response_lower.count(topic) for topic in topic_list}\n",
        "\n",
        "# Print the counts\n",
        "print(\"Topic counts:\", topic_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHks6G0pXSRB",
        "outputId": "0514022c-e507-4d9a-a4aa-957d867d5cb5"
      },
      "id": "KHks6G0pXSRB",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Government survey, Public sector employees, Satisfaction rating, NASA, Popular department\n",
            "Topic counts: {'nasa': 1, 'local government': 0, 'engineering': 0, 'employee satisfaction': 0, 'federal government': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "oRxBeEYlXaTj"
      },
      "execution_count": null,
      "outputs": [],
      "id": "oRxBeEYlXaTj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert topic_counts to markdown format\n",
        "markdown_output = \"\\n\".join(f\"- **{topic}**: {count}\" for topic, count in topic_counts.items())\n",
        "\n",
        "# Display the markdown output\n",
        "display(Markdown(markdown_output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "60efd7d5-f572-4869-8fbc-6b6c0a3babf7",
        "id": "soTbNyqYXaTk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- **nasa**: 1\n- **local government**: 0\n- **engineering**: 0\n- **employee satisfaction**: 0\n- **federal government**: 0"
          },
          "metadata": {}
        }
      ],
      "id": "soTbNyqYXaTk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: this is referred as **zero-shot prompting**, that is without task training, The model hasn't been fine-tuned or trained on specific examples of the task. It relies on its pre-trained knowledge base."
      ],
      "metadata": {
        "id": "bpmxSHpZmZ2m"
      },
      "id": "bpmxSHpZmZ2m"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if there is a new NASA story and print alert if true\n",
        "if topic_counts['nasa'] == 1:\n",
        "    print(\"ALERT: New NASA story!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f41a2859-fe8c-486e-d07a-434ecb612794",
        "id": "rzS6JEBhXaTk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALERT: New NASA story!\n"
          ]
        }
      ],
      "id": "rzS6JEBhXaTk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: from the ouput above, the news article `story` can be classified as : Nasa, employee's satisfaction, and government."
      ],
      "metadata": {
        "id": "tg-R2kynlfLi"
      },
      "id": "tg-R2kynlfLi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a JSON format instead of a list"
      ],
      "metadata": {
        "id": "zJpdHT9yYOBV"
      },
      "id": "zJpdHT9yYOBV"
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "10UbCIymYynw"
      },
      "id": "10UbCIymYynw",
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt for the language model\n",
        "prompt = f\"\"\"\n",
        "Determine five topics that are being discussed in the \\\n",
        "following text, which is delimited by triple backticks.\n",
        "\n",
        "Make each item one or two words long.\n",
        "\n",
        "Format your response as a JSON object with the topics as keys and their counts as values.\n",
        "\n",
        "Text sample: '''{story}'''\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "w3hr6ozNYnFs"
      },
      "id": "w3hr6ozNYnFs",
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the response from the language model\n",
        "response = get_completion(prompt)\n",
        "print(\"Response:\", response)\n",
        "\n",
        "# Convert the response to a dictionary\n",
        "topic_counts = json.loads(response)\n",
        "\n",
        "# Convert topic_counts to markdown format\n",
        "markdown_output = \"\\n\".join(f\"- **{topic}**: {count}\" for topic, count in topic_counts.items())\n",
        "\n",
        "# Display the markdown output\n",
        "display(Markdown(markdown_output))\n",
        "\n",
        "# Check if there is a new NASA story and print alert if true\n",
        "if topic_counts.get('nasa', 0) == 1:\n",
        "    print(\"ALERT: New NASA story!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "2goXkQWsYuCK",
        "outputId": "c342f867-37c0-4866-9672-2a0e544b262c"
      },
      "id": "2goXkQWsYuCK",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: {\n",
            "  \"survey\": 1,\n",
            "  \"government\": 1,\n",
            "  \"public sector\": 1,\n",
            "  \"NASA\": 1,\n",
            "  \"satisfaction rating\": 1\n",
            "}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- **survey**: 1\n- **government**: 1\n- **public sector**: 1\n- **NASA**: 1\n- **satisfaction rating**: 1"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f88408ae-469a-4b02-a043-f6b4f0b14bf9",
      "metadata": {
        "id": "f88408ae-469a-4b02-a043-f6b4f0b14bf9"
      },
      "source": [
        "## Try experimenting on your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1bd3553f",
      "metadata": {
        "height": 30,
        "id": "1bd3553f"
      },
      "outputs": [],
      "source": [
        "text = f\"\"\"\n",
        "Despite the setbacks, Jane was determined to finish the project on time. She stayed late every night and worked through weekends.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "based on ```{text}```,  what can you infer about Jane's attitude towards the project?\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "pprint(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xsnP9DBWQAH",
        "outputId": "bdc42fee-b2be-42e9-91a0-b6a1bc9c8bf1"
      },
      "id": "8xsnP9DBWQAH",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('From the given statement, it can be inferred that Jane is very committed and '\n",
            " 'dedicated to the project. Despite facing setbacks, she remains determined to '\n",
            " 'complete the project on time. Her willingness to stay late every night and '\n",
            " 'work through weekends shows that she is highly motivated and focused on '\n",
            " \"achieving her goal. Overall, Jane's attitude towards the project can be \"\n",
            " 'described as determined, hardworking, and persistent.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N_taw1GFd8iE"
      },
      "id": "N_taw1GFd8iE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identifying a sentiment - and making a choice between positive or negative sentiment\n",
        "prompt = f\"\"\"\n",
        "based on ```{text}```,  what can you infer about Jane's attitude towards the project? What sentiment can you suggest.\n",
        "Give only one sentiment.\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "pprint(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d72f5179-1d1a-4d69-f764-ca9b54a37e67",
        "id": "KMBkq6f4dHRD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Dedication.'\n"
          ]
        }
      ],
      "id": "KMBkq6f4dHRD"
    },
    {
      "cell_type": "code",
      "source": [
        "# identifying a sentiment\n",
        "prompt = f\"\"\"\n",
        "based on ```{text}```,  what can you infer about Jane's attitude towards the project?\n",
        "Identify a list of emotions in bullet points.\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "pprint(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e7f89a-1f45-4e4c-827e-8b6dab34eb9c",
        "id": "Ia5M7ls4bgx7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'- Determination\\n- Perseverance\\n- Commitment\\n- Dedication'\n"
          ]
        }
      ],
      "id": "Ia5M7ls4bgx7"
    },
    {
      "cell_type": "code",
      "source": [
        "type(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg1rcjcrg059",
        "outputId": "2bb40bb2-9393-41bc-c51e-1a8287ba2a45"
      },
      "id": "dg1rcjcrg059",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "readable_output = \"\\n\".join(f\"- {item}\" for item in response)"
      ],
      "metadata": {
        "id": "EiHM_myeg5n1"
      },
      "id": "EiHM_myeg5n1",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(readable_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyKgzW7sgVhG",
        "outputId": "0b36a295-55f6-4132-e764-5027da0771d4"
      },
      "id": "jyKgzW7sgVhG",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "# Display the output as markdown\n",
        "display(Markdown(readable_output))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "KFhBXVyChY1w",
        "outputId": "f5e9c874-7405-4920-f3b3-fa0d79eeae91"
      },
      "id": "KFhBXVyChY1w",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Determination\n- Perseverance\n- Commitment\n- Dedication"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ng17IuLnfbvW"
      },
      "id": "ng17IuLnfbvW",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}