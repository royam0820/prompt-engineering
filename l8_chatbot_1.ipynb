{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royam0820/prompt-engineering/blob/main/l8_chatbot_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9183228-0ba6-4af9-8430-649e28868253",
      "metadata": {
        "id": "a9183228-0ba6-4af9-8430-649e28868253"
      },
      "source": [
        "# The Chat Format\n",
        "\n",
        "In this notebook, you will explore how you can utilize the chat format to have extended conversations with chatbots personalized or specialized for specific tasks or behaviors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "Pu9wW-jAxxeB"
      },
      "id": "Pu9wW-jAxxeB"
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install openai==0.27\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdI_qEYjx2vv",
        "outputId": "c1ed7ab5-30e2-4fc8-e229-ad062d0c9b86"
      },
      "id": "UdI_qEYjx2vv",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.35.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "F6RSPP7-yOI4"
      },
      "id": "F6RSPP7-yOI4",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "9f_jhrtiyTOJ"
      },
      "id": "9f_jhrtiyTOJ",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(openai.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgwdyOeeEH1o",
        "outputId": "1a8340fe-b9aa-4abd-87f0-6e696a00c571"
      },
      "id": "ZgwdyOeeEH1o",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.35.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ],
      "metadata": {
        "id": "cArT1i58yvYW"
      },
      "id": "cArT1i58yvYW",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\",temperature=0):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,   # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    # Access the content directly from the 'message' attribute\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "ZRj8ktP-pooV"
      },
      "execution_count": 6,
      "outputs": [],
      "id": "ZRj8ktP-pooV"
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing the OpenAI client\n",
        "client = openai.OpenAI()\n",
        "\n",
        "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "vuKyBjfjOqnS"
      },
      "id": "vuKyBjfjOqnS",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: The line `client=openai.OpenAI()` initializes the OpenAI client.  It typically means creating an instance of a class that handles the details of communicating with the API. However, the OpenAI Python client library simplifies this by allowing direct calls without explicitly initializing a client object. If your setup requires this line, it indicates that the client object is used to handle API interactions. This isn't standard for most setups, so check your environment or any custom configurations."
      ],
      "metadata": {
        "id": "SP0dYjizQmv7"
      },
      "id": "SP0dYjizQmv7"
    },
    {
      "cell_type": "code",
      "source": [
        "# client = openai.OpenAI()\n",
        "\n",
        "# def get_completion_from_messages(messages,\n",
        "#                                  model=\"gpt-3.5-turbo\",\n",
        "#                                  temperature=0,\n",
        "#                                  max_tokens=500):\n",
        "#     response = client.chat.completions.create(model=model,\n",
        "#     messages=messages,\n",
        "#     temperature=temperature,\n",
        "#     max_tokens=max_tokens)\n",
        "#     return response.choices[0].message.content\n",
        "\n"
      ],
      "metadata": {
        "id": "twkL81xRMUbn"
      },
      "execution_count": 8,
      "outputs": [],
      "id": "twkL81xRMUbn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: with the above commented code, you can a `max_tokens` parameter. It is often useful to include it to control the length of the response and avoid excessive usage of tokens which could result in higher costs or unexpected behavior."
      ],
      "metadata": {
        "id": "45lWPR_VOwUJ"
      },
      "id": "45lWPR_VOwUJ"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cee681b7",
      "metadata": {
        "height": 98,
        "tags": [],
        "id": "cee681b7"
      },
      "outputs": [],
      "source": [
        "# initialising messages\n",
        "messages =  [\n",
        "{'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},\n",
        "{'role':'user', 'content':'tell me a joke'},\n",
        "{'role':'assistant', 'content':'Why did the chicken cross the road'},\n",
        "{'role':'user', 'content':'I don\\'t know'}  ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: In a conversational AI system, the roles \"system,\" \"user,\" and \"assistant\" define the context and interaction flow. Hereâ€™s an explanation of each role based on the provided code:\n",
        "- **The system role**: The \"system\" role sets the initial instructions or context for the assistant. It helps to define the behavior, style, or rules that the assistant should follow throughout the conversation.\n",
        "- **The user role**: The \"user\" role represents the inputs or queries made by the person interacting with the assistant. These inputs drive the conversation and determine the assistant's subsequent responses.\n",
        "- **The assistant role**: The \"assistant\" role represents the AI or conversational agent's responses to the user's inputs. The responses are based on the instructions given by the \"system\" and the inputs from the \"user.\""
      ],
      "metadata": {
        "id": "C292GHUuTHyH"
      },
      "id": "C292GHUuTHyH"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "da45ea0f",
      "metadata": {
        "height": 47,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da45ea0f",
        "outputId": "6c0a512e-25c0-4850-83f8-d5971fd56c97",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verily, the fowl crossed the thoroughfare to reach the other side, forsooth!\n"
          ]
        }
      ],
      "source": [
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: Shakespearean answer! In plain English, it will be: \"To get to the other side! It's a joke that always makes people laugh.\"\n",
        "\n",
        "The joke \"Why did the chicken cross the road? To get to the other side!\" is a classic example of an anti-joke. An anti-joke is a type of humor where the punchline is deliberately mundane or straightforward rather than a typical witty or humorous twist. The humor comes from subverting the listener's expectations.\n",
        "\n",
        "\"L'Ã©nigme 'Pourquoi le poulet a-t-il traversÃ© la route ? Pour aller de l'autre cÃ´tÃ© !' est un exemple classique de blague anti-humoristique. Une blague anti-humoristique est un type d'humour oÃ¹ la chute est dÃ©libÃ©rÃ©ment banale ou directe plutÃ´t que spirituelle ou humoristique. L'humour vient de la subversion des attentes de l'auditeur.\""
      ],
      "metadata": {
        "id": "IsApOn5BWU03"
      },
      "id": "IsApOn5BWU03"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ca733f8f",
      "metadata": {
        "height": 98,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca733f8f",
        "outputId": "55fde65f-467f-4110-b0d4-4a8c57df498f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Isa! It's nice to meet you. How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "messages =  [\n",
        "{'role':'system', 'content':'You are friendly chatbot.'},\n",
        "{'role':'user', 'content':'Hi, my name is Isa'}  ]\n",
        "response = get_completion_from_messages(messages, temperature=1) # higher temperature\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: Asssitant response."
      ],
      "metadata": {
        "id": "OAnU4_nLV6BD"
      },
      "id": "OAnU4_nLV6BD"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0ae595bc",
      "metadata": {
        "height": 98,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ae595bc",
        "outputId": "ee0421aa-53dc-4f57-c41d-ad31e7167abf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but as a chatbot, I do not have the ability to know your name. Feel free to introduce yourself if you'd like!\n"
          ]
        }
      ],
      "source": [
        "messages =  [\n",
        "{'role':'system', 'content':'You are friendly chatbot.'},\n",
        "{'role':'user', 'content':'Yes,  can you remind me, What is my name?'}  ]\n",
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: The model does not remember the name of the user.  Each conversaion with a language model is a standalone interaction which means that you must provide all relevant messages for the model to draw from in the current conversation. To have the model remember past messages, you must provide them, see below."
      ],
      "metadata": {
        "id": "Su9vj9kSanRW"
      },
      "id": "Su9vj9kSanRW"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "56cbb817",
      "metadata": {
        "height": 149,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56cbb817",
        "outputId": "bd1de306-ab53-449e-f3bc-4bcab3744532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your name is Isa. It's a pleasure to chat with you!\n"
          ]
        }
      ],
      "source": [
        "messages =  [\n",
        "{'role':'system', 'content':'You are friendly chatbot.'},\n",
        "{'role':'user', 'content':'Hi, my name is Isa'},\n",
        "{'role':'assistant', 'content': \"Hi Isa! It's nice to meet you. \\\n",
        "Is there anything I can help you with today?\"},\n",
        "{'role':'user', 'content':'Yes, you can remind me, What is my name?'}  ]\n",
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: the model was able to respond because it has all the context it needs to recall the user's name."
      ],
      "metadata": {
        "id": "s-FxX2MWbspW"
      },
      "id": "s-FxX2MWbspW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Chatbot (new amr)"
      ],
      "metadata": {
        "id": "oj1tZwQLrGJE"
      },
      "id": "oj1tZwQLrGJE"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f37f1e-d3fc-4a08-9415-084b9d5ea234",
        "collapsed": true,
        "id": "x0sJ9hgX5HOg"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.36.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.111.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==1.0.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.0.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.10)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.1->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.1->gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.0.4)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (5.10.0)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (2.2.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.22.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "id": "x0sJ9hgX5HOg"
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = openai.OpenAI()\n",
        "\n",
        "# Initialize conversation history\n",
        "conversation_history = [\n",
        "    {'role':'system', 'content':\"\"\"\n",
        "    You are OrderBot, an automated service to collect orders for a pizza restaurant.\n",
        "    You first greet the customer, then collects the order,\n",
        "    and then asks if it's a pickup or delivery.\n",
        "    You wait to collect the entire order, then summarize it and check for a final\n",
        "    time if the customer wants to add anything else.\n",
        "    If it's a delivery, you ask for an address.\n",
        "    Finally you collect the payment.\n",
        "    Make sure to clarify all options, extras and sizes to uniquely\n",
        "    identify the item from the menu.\n",
        "    You respond in a short, very conversational friendly style.\n",
        "    The menu includes\n",
        "    pepperoni pizza 12.95, 10.00, 7.00\n",
        "    cheese pizza 10.95, 9.25, 6.50\n",
        "    eggplant pizza 11.95, 9.75, 6.75\n",
        "    fries 4.50, 3.50\n",
        "    greek salad 7.25\n",
        "    Toppings:\n",
        "    extra cheese 2.00,\n",
        "    mushrooms 1.50\n",
        "    sausage 3.00\n",
        "    canadian bacon 3.50\n",
        "    AI sauce 1.50\n",
        "    peppers 1.00\n",
        "    Drinks:\n",
        "    coke 3.00, 2.00, 1.00\n",
        "    sprite 3.00, 2.00, 1.00\n",
        "    bottled water 5.00\n",
        "    \"\"\"}\n",
        "]\n",
        "\n",
        "def chatbot(message, history):\n",
        "    global conversation_history\n",
        "\n",
        "    # Add user message to conversation history\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Get response from OpenAI\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=conversation_history\n",
        "    )\n",
        "\n",
        "    ai_message = response.choices[0].message.content\n",
        "\n",
        "    # Add AI response to conversation history\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": ai_message})\n",
        "\n",
        "    # Return the full conversation history\n",
        "    return [(conversation_history[i][\"content\"], conversation_history[i+1][\"content\"])\n",
        "            for i in range(1, len(conversation_history)-1, 2)]\n",
        "\n",
        "# Function to initialize the chat with a greeting\n",
        "def init_chat():\n",
        "    global conversation_history\n",
        "\n",
        "    # Get initial greeting from OpenAI\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=conversation_history\n",
        "    )\n",
        "\n",
        "    greeting = response.choices[0].message.content\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": greeting})\n",
        "\n",
        "    return [(\"\", greeting)]\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot,\n",
        "    inputs=gr.Textbox(placeholder=\"Type your message here...\"),\n",
        "    outputs=gr.Chatbot(height=500),\n",
        "    title=\"Pizza OrderBot\",\n",
        "    description=\"I'm here to help you order pizza. What would you like?\",\n",
        "    allow_flagging=\"never\",\n",
        ")\n",
        "\n",
        "# Launch the interface with the initial greeting\n",
        "iface.launch(lambda: init_chat())#"
      ],
      "metadata": {
        "id": "xsX4K7FI3CFC"
      },
      "id": "xsX4K7FI3CFC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 166,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f352f844-4b5c-4fa9-febf-17f99f0e6d83",
        "id": "vaV5NqZq6RyW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"pizza\": {\n",
            "    \"type\": \"cheese pizza\",\n",
            "    \"size\": \"large\",\n",
            "    \"price\": 10.95\n",
            "  },\n",
            "  \"toppings\": [],\n",
            "  \"drinks\": [],\n",
            "  \"sides\": [],\n",
            "  \"total price\": 10.95\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# this code displayed the whole message conversation in a JSON format\n",
        "messages =  context.copy()\n",
        "messages.append(\n",
        "{'role':'system', 'content':'create a json summary of the previous food order. Itemize the price for each item\\\n",
        " The fields should be 1) pizza, include size 2) list of toppings 3) list of drinks, include size   4) list of sides include size  5)total price '},\n",
        ")\n",
        " #The fields should be 1) pizza, price 2) list of toppings 3) list of drinks, include size include price  4) list of sides include size include price, 5)total price '},\n",
        "\n",
        "response = get_completion_from_messages(messages, temperature=0)\n",
        "print(response)"
      ],
      "id": "vaV5NqZq6RyW"
    },
    {
      "cell_type": "markdown",
      "id": "dedba66a-58b0-40d4-b9ae-47e79ae22328",
      "metadata": {
        "id": "dedba66a-58b0-40d4-b9ae-47e79ae22328"
      },
      "source": [
        "# Panel OrderBot (orig)\n",
        "We can automate the collection of user prompts and assistant responses to build a  OrderBot. The OrderBot will take orders at a pizza restaurant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e76749ac",
      "metadata": {
        "height": 234,
        "tags": [],
        "id": "e76749ac"
      },
      "outputs": [],
      "source": [
        "def collect_messages(_):\n",
        "    prompt = inp.value_input\n",
        "    inp.value = ''\n",
        "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
        "    response = get_completion_from_messages(context)\n",
        "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
        "    panels.append(\n",
        "        pn.Row('User:', pn.pane.Markdown(prompt, width=600)))\n",
        "    panels.append(\n",
        "        pn.Row('Assistant:', pn.pane.Markdown(response, width=600, style={'background-color': '#F6F6F6'})))\n",
        "\n",
        "    return pn.Column(*panels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: The code above ia an helper function, we will collect all the conversations beween the user and the model (the assistant) from the user interface.\n",
        "\n",
        "The function `collect_messages` captures user input, updates the conversation context, generates a response from the assistant, and updates the UI to reflect the conversation. The UI components are managed using `pn` (presumably panel library) for creating interactive web applications."
      ],
      "metadata": {
        "id": "PQp0r_zWcHdd"
      },
      "id": "PQp0r_zWcHdd"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tmdj4u3inJVc"
      },
      "id": "tmdj4u3inJVc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 829,
        "tags": [],
        "id": "474b557c"
      },
      "outputs": [],
      "source": [
        "import panel as pn  # GUI\n",
        "pn.extension()\n",
        "\n",
        "panels = [] # collect display\n",
        "\n",
        "context = [ {'role':'system', 'content':\"\"\"\n",
        "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
        "You first greet the customer, then collects the order, \\\n",
        "and then asks if it's a pickup or delivery. \\\n",
        "You wait to collect the entire order, then summarize it and check for a final \\\n",
        "time if the customer wants to add anything else. \\\n",
        "If it's a delivery, you ask for an address. \\\n",
        "Finally you collect the payment.\\\n",
        "Make sure to clarify all options, extras and sizes to uniquely \\\n",
        "identify the item from the menu.\\\n",
        "You respond in a short, very conversational friendly style. \\\n",
        "The menu includes \\\n",
        "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
        "cheese pizza   10.95, 9.25, 6.50 \\\n",
        "eggplant pizza   11.95, 9.75, 6.75 \\\n",
        "fries 4.50, 3.50 \\\n",
        "greek salad 7.25 \\\n",
        "Toppings: \\\n",
        "extra cheese 2.00, \\\n",
        "mushrooms 1.50 \\\n",
        "sausage 3.00 \\\n",
        "canadian bacon 3.50 \\\n",
        "AI sauce 1.50 \\\n",
        "peppers 1.00 \\\n",
        "Drinks: \\\n",
        "coke 3.00, 2.00, 1.00 \\\n",
        "sprite 3.00, 2.00, 1.00 \\\n",
        "bottled water 5.00 \\\n",
        "\"\"\"} ]  # accumulate messages\n",
        "\n",
        "\n",
        "inp = pn.widgets.TextInput(value=\"Hi\", placeholder='Enter text hereâ€¦')\n",
        "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
        "\n",
        "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
        "\n",
        "dashboard = pn.Column(\n",
        "    inp,\n",
        "    pn.Row(button_conversation),\n",
        "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
        ")\n",
        "\n",
        "dashboard"
      ],
      "id": "474b557c"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a0ee11b9",
      "metadata": {
        "height": 166,
        "id": "a0ee11b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f352f844-4b5c-4fa9-febf-17f99f0e6d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"pizza\": {\n",
            "    \"type\": \"cheese pizza\",\n",
            "    \"size\": \"large\",\n",
            "    \"price\": 10.95\n",
            "  },\n",
            "  \"toppings\": [],\n",
            "  \"drinks\": [],\n",
            "  \"sides\": [],\n",
            "  \"total price\": 10.95\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# this code displayed the whole message conversation\n",
        "messages =  context.copy()\n",
        "messages.append(\n",
        "{'role':'system', 'content':'create a json summary of the previous food order. Itemize the price for each item\\\n",
        " The fields should be 1) pizza, include size 2) list of toppings 3) list of drinks, include size   4) list of sides include size  5)total price '},\n",
        ")\n",
        " #The fields should be 1) pizza, price 2) list of toppings 3) list of drinks, include size include price  4) list of sides include size include price, 5)total price '},\n",
        "\n",
        "response = get_completion_from_messages(messages, temperature=0)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3153c581-1c72-497a-9293-8db3bcb804fc",
      "metadata": {
        "id": "3153c581-1c72-497a-9293-8db3bcb804fc"
      },
      "source": [
        "## Try experimenting on your own!\n",
        "\n",
        "You can modify the menu or instructions to create your own orderbot!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimenting on role attribution"
      ],
      "metadata": {
        "id": "L1gkoyu3T6SD"
      },
      "id": "L1gkoyu3T6SD"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2cc84122",
      "metadata": {
        "height": 30,
        "id": "2cc84122",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5e9a74-79e3-46f4-f886-3f26490129b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'role': 'system',\n",
              " 'content': 'You are an assistant that speaks like Shakespeare.'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# system role example\n",
        "{'role': 'system', 'content': 'You are an assistant that speaks like Shakespeare.'}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: This instructs the assistant to adopt a Shakespearean style in its responses."
      ],
      "metadata": {
        "id": "Iv3gQLH4Uljs"
      },
      "id": "Iv3gQLH4Uljs"
    },
    {
      "cell_type": "code",
      "source": [
        "# user role example\n",
        "{'role': 'user', 'content': 'tell me a joke'}\n",
        "{'role': 'user', 'content': 'I don\\'t know'}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHJWIbAFUP8F",
        "outputId": "15e49d93-0f31-4df5-9a8e-5f7976ef66fd"
      },
      "id": "lHJWIbAFUP8F",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'role': 'user', 'content': \"I don't know\"}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: These show the user asking for a joke and then responding to the assistant's joke setup"
      ],
      "metadata": {
        "id": "dpYUayzDUrM3"
      },
      "id": "dpYUayzDUrM3"
    },
    {
      "cell_type": "code",
      "source": [
        "# assitant role exemple\n",
        "{'role': 'assistant', 'content': 'Why did the chicken cross the road'}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVlqfxGgU4sp",
        "outputId": "086602d4-ed97-4844-a492-4fd7e7f77b49"
      },
      "id": "sVlqfxGgU4sp",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'role': 'assistant', 'content': 'Why did the chicken cross the road'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: Here, the assistant responds to the user's request for a joke by starting a classic joke."
      ],
      "metadata": {
        "id": "LzBETzWLVLU8"
      },
      "id": "LzBETzWLVLU8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "These roles collectively structure the dialogue, enabling the assistant to understand its behavior (as dictated by the system), respond to user queries, and maintain a coherent conversation flow."
      ],
      "metadata": {
        "id": "abY5SXunVQJG"
      },
      "id": "abY5SXunVQJG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### My Travel Chatbot"
      ],
      "metadata": {
        "id": "DPiqEXAj6rQW"
      },
      "id": "DPiqEXAj6rQW"
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = openai.OpenAI()\n",
        "\n",
        "# Initialize conversation history\n",
        "conversation_history = [\n",
        "    {'role':'system', 'content':\"\"\"\n",
        "    You are TravelBot, an automated travel planning assistant.\n",
        "    You help users plan their vacations by suggesting destinations, activities, and accommodations.\n",
        "    First, greet the user and ask about their travel preferences (e.g., budget, desired climate, type of trip).\n",
        "    Then, suggest destinations based on their preferences.\n",
        "    Once a destination is chosen, recommend activities and attractions.\n",
        "    Ask about the length of stay and suggest appropriate accommodations.\n",
        "    Finally, summarize the travel plan and ask if they want to make any changes.\n",
        "    You respond in a friendly, enthusiastic, and informative style.\n",
        "    You have knowledge of popular destinations worldwide, including:\n",
        "\n",
        "    Beach destinations: Bali, Hawaii, Maldives, Caribbean Islands\n",
        "    City breaks: Paris, New York, Tokyo, London\n",
        "    Nature and adventure: Costa Rica, New Zealand, Switzerland, Canada\n",
        "    Cultural experiences: Italy, Japan, India, Egypt\n",
        "\n",
        "    For each destination, you can suggest:\n",
        "    - Popular attractions and activities\n",
        "    - Typical weather and best times to visit\n",
        "    - Accommodation options (budget hostels to luxury resorts)\n",
        "    - Local cuisine and dining recommendations\n",
        "    - Transportation options\n",
        "    - Estimated costs for various aspects of the trip\n",
        "    \"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def chatbot(message, history):\n",
        "    global conversation_history\n",
        "\n",
        "    # Add user message to conversation history\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Get response from OpenAI\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=conversation_history\n",
        "    )\n",
        "\n",
        "    ai_message = response.choices[0].message.content\n",
        "\n",
        "    # Add AI response to conversation history\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": ai_message})\n",
        "\n",
        "    # Return the full conversation history\n",
        "    return [(conversation_history[i][\"content\"], conversation_history[i+1][\"content\"])\n",
        "            for i in range(1, len(conversation_history)-1, 2)]\n",
        "\n",
        "# Function to initialize the chat with a greeting\n",
        "def init_chat():\n",
        "    global conversation_history\n",
        "\n",
        "    # Get initial greeting from OpenAI\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=conversation_history\n",
        "    )\n",
        "\n",
        "    greeting = response.choices[0].message.content\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": greeting})\n",
        "\n",
        "    return [(\"\", greeting)]\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot,\n",
        "    inputs=gr.Textbox(placeholder=\"Type your travel preferences or questions here...\"),\n",
        "    outputs=gr.Chatbot(height=500),\n",
        "    title=\"TravelBot - Your Personal Travel Planner\",\n",
        "    description=\"I'm here to help you plan your perfect vacation. What kind of trip are you looking for?\",\n",
        "    allow_flagging=\"never\",\n",
        ")\n",
        "\n",
        "# Launch the interface with the initial greeting\n",
        "iface.launch(lambda: init_chat())"
      ],
      "metadata": {
        "id": "rzYLVEoP7Mp0"
      },
      "id": "rzYLVEoP7Mp0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = openai.OpenAI()\n",
        "\n",
        "# Initialize conversation history\n",
        "conversation_history = [\n",
        "    {'role':'system', 'content':\"\"\"\n",
        "    You are TravelBot, an automated travel planning assistant.\n",
        "    You help users plan their vacations by suggesting destinations, activities, and accommodations.\n",
        "    First, greet the user and ask about their travel preferences (e.g., budget, desired climate, type of trip).\n",
        "    Then, suggest destinations based on their preferences.\n",
        "    Once a destination is chosen, recommend activities and attractions.\n",
        "    Ask about the length of stay and suggest appropriate accommodations.\n",
        "    Finally, summarize the travel plan and ask if they want to make any changes.\n",
        "    You respond in a friendly, enthusiastic, and informative style.\n",
        "    You have knowledge of popular destinations worldwide, including:\n",
        "\n",
        "    Beach destinations: Bali, Hawaii, Maldives, Caribbean Islands\n",
        "    City breaks: Paris, New York, Tokyo, London\n",
        "    Nature and adventure: Costa Rica, New Zealand, Switzerland, Canada\n",
        "    Cultural experiences: Italy, Japan, India, Egypt\n",
        "\n",
        "    For each destination, you can suggest:\n",
        "    - Popular attractions and activities\n",
        "    - Typical weather and best times to visit\n",
        "    - Accommodation options (budget hostels to luxury resorts)\n",
        "    - Local cuisine and dining recommendations\n",
        "    - Transportation options\n",
        "    - Estimated costs for various aspects of the trip\n",
        "    \"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def chatbot(message, history):\n",
        "    global conversation_history\n",
        "\n",
        "    # Add user message to conversation history\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Get response from OpenAI\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=conversation_history\n",
        "    )\n",
        "\n",
        "    ai_message = response.choices[0].message.content\n",
        "\n",
        "    # Add AI response to conversation history\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": ai_message})\n",
        "\n",
        "    # Return the last exchange\n",
        "    return ai_message\n",
        "\n",
        "# Function to initialize the chat with a greeting\n",
        "def init_chat():\n",
        "    global conversation_history\n",
        "\n",
        "    # Get initial greeting from OpenAI\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=conversation_history\n",
        "    )\n",
        "\n",
        "    greeting = response.choices[0].message.content\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": greeting})\n",
        "\n",
        "    return [(\"\", greeting)]\n",
        "\n",
        "# Function to clear the input field\n",
        "def clear_input():\n",
        "    return \"\"\n",
        "\n",
        "with gr.Blocks() as iface:\n",
        "    chatbot_component = gr.Chatbot(height=500)\n",
        "    msg = gr.Textbox(placeholder=\"Type your travel preferences or questions here...\")\n",
        "    clear = gr.Button(\"Clear\")\n",
        "\n",
        "    def user(user_message, history):\n",
        "        return \"\", history + [[user_message, None]]\n",
        "\n",
        "    def bot(history):\n",
        "        user_message = history[-1][0]\n",
        "        bot_message = chatbot(user_message, history)\n",
        "        history[-1][1] = bot_message\n",
        "        return history\n",
        "\n",
        "    msg.submit(user, [msg, chatbot_component], [msg, chatbot_component]).then(\n",
        "        bot, chatbot_component, chatbot_component\n",
        "    )\n",
        "    clear.click(clear_input, outputs=msg)\n",
        "\n",
        "    iface.load(init_chat, outputs=chatbot_component)\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "collapsed": true,
        "id": "6FMQU6XG8S2K",
        "outputId": "3fd0609b-3b03-4b08-f4d5-1d2a42f6690e"
      },
      "id": "6FMQU6XG8S2K",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://e084ad90b53920cc3b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e084ad90b53920cc3b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jM1PrHzT9RjE"
      },
      "id": "jM1PrHzT9RjE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying a complete summary of the travel bot conversation."
      ],
      "metadata": {
        "id": "juH3k25h-Zcu"
      },
      "id": "juH3k25h-Zcu"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "height": 166,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "81208785-48e1-484c-95de-9f614d203e00",
        "id": "-xv0dKXT9ea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://3f72b41a57817050c7.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3f72b41a57817050c7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def get_travel_summary():\n",
        "    global conversation_history\n",
        "\n",
        "    # Add a system message to request a JSON summary\n",
        "    conversation_history.append(\n",
        "        {'role':'system', 'content':'''\n",
        "        Create a JSON summary of the travel plan discussed. Include the following fields:\n",
        "        1) destination\n",
        "        2) duration (in days)\n",
        "        3) accommodations (list with prices if mentioned)\n",
        "        4) activities (list with prices if mentioned)\n",
        "        5) transportation (list with prices if mentioned)\n",
        "        6) estimated total budget\n",
        "\n",
        "        If any information is not available, use \"Not specified\" as the value.\n",
        "        '''}\n",
        "    )\n",
        "\n",
        "    # Get response from OpenAI\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=conversation_history\n",
        "    )\n",
        "\n",
        "    summary = response.choices[0].message.content\n",
        "\n",
        "    # Remove the summary request from conversation history\n",
        "    conversation_history.pop()\n",
        "\n",
        "    try:\n",
        "        # Parse the JSON response\n",
        "        summary_json = json.loads(summary)\n",
        "        return summary_json\n",
        "    except json.JSONDecodeError:\n",
        "        return {\"error\": \"Failed to generate a valid JSON summary\"}\n",
        "\n",
        "# Add this function to your Gradio interface\n",
        "def display_summary():\n",
        "    summary = get_travel_summary()\n",
        "    if \"error\" in summary:\n",
        "        return summary[\"error\"]\n",
        "    else:\n",
        "        return json.dumps(summary, indent=2)\n",
        "\n",
        "# Modify your Gradio interface to include a summary button\n",
        "with gr.Blocks() as iface:\n",
        "    chatbot_component = gr.Chatbot(height=500)\n",
        "    msg = gr.Textbox(placeholder=\"Type your travel preferences or questions here...\")\n",
        "    clear = gr.Button(\"Clear\")\n",
        "    summary_button = gr.Button(\"Get Travel Summary\")\n",
        "    summary_output = gr.Textbox(label=\"Travel Summary\", lines=10)\n",
        "\n",
        "    def user(user_message, history):\n",
        "        return \"\", history + [[user_message, None]]\n",
        "\n",
        "    def bot(history):\n",
        "        user_message = history[-1][0]\n",
        "        bot_message = chatbot(user_message, history)\n",
        "        history[-1][1] = bot_message\n",
        "        return history\n",
        "\n",
        "    msg.submit(user, [msg, chatbot_component], [msg, chatbot_component]).then(\n",
        "        bot, chatbot_component, chatbot_component\n",
        "    )\n",
        "    clear.click(clear_input, outputs=msg)\n",
        "    summary_button.click(display_summary, outputs=summary_output)\n",
        "\n",
        "    iface.load(init_chat, outputs=chatbot_component)\n",
        "\n",
        "iface.launch()"
      ],
      "id": "-xv0dKXT9ea0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}